{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIltdJD0xYT1"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from Bio.Blast import NCBIXML\n",
        "import csv\n",
        "from Bio import SeqIO\n",
        "\n",
        "query_fasta = '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_msa_filt_proteins.fasta'\n",
        "# Path to the output XML file\n",
        "blast_output = \"/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_blast_output.xml\"\n",
        "# Path to BLAST database\n",
        "database = \"/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/blast_db/at_db_adj\"\n",
        "results_csv = \"/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_blast_output_adj.csv\"\n",
        "e_value_threshold = 0.005\n",
        "\n",
        "# Construct the BLAST command as a list\n",
        "blast_command = [\n",
        "    '/home/adititm/ncbi-blast-2.15.0+/bin/blastp',\n",
        "    '-query', query_fasta,\n",
        "    '-db', database,\n",
        "    '-evalue', str(e_value_threshold),\n",
        "    '-outfmt', '5',  # XML format\n",
        "    '-out', blast_output,\n",
        "    '-num_threads', '2'\n",
        "]\n",
        "\n",
        "# Run the BLAST command\n",
        "subprocess.run(blast_command, check=True)\n",
        "\n",
        "# Parse query sequences into a dictionary\n",
        "query_sequences = {record.id: str(record.seq) for record in SeqIO.parse(query_fasta, \"fasta\")}\n",
        "\n",
        "# Open the BLAST XML output for parsing within a single continuous context\n",
        "with open(blast_output) as result_handle:\n",
        "    blast_records = NCBIXML.parse(result_handle)\n",
        "\n",
        "    # Open a CSV file to write the results\n",
        "    with open(results_csv, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['Query ID', 'Query Sequence', 'Subject', 'E-value', 'Score', 'Align Length', 'Identities', 'Match']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "        # Write the header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Process BLAST results and write to CSV\n",
        "        for blast_record in blast_records:\n",
        "            query_id = blast_record.query.split()[0]  # Assumes the first word in the query description is the ID\n",
        "            query_seq = query_sequences.get(query_id, \"NA\")  # Get the sequence or NA if not found\n",
        "            for alignment in blast_record.alignments:\n",
        "                for hsp in alignment.hsps:\n",
        "                    if hsp.expect < e_value_threshold:\n",
        "                        writer.writerow({\n",
        "                            'Query ID': query_id,\n",
        "                            'Query Sequence': query_seq,\n",
        "                            'Subject': alignment.title.replace(',', ';'),  # Replace commas in subject title\n",
        "                            'E-value': hsp.expect,\n",
        "                            'Score': hsp.score,\n",
        "                            'Align Length': hsp.align_length,\n",
        "                            'Identities': hsp.identities,\n",
        "                            'Match': hsp.match.replace(',', ';')  # Replace commas in matches\n",
        "                        })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "import csv\n",
        "import shutil\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "from Bio.Blast import NCBIXML\n",
        "from Bio.Blast import NCBIWWW\n",
        "from Bio import Align\n",
        "from Bio.PDB import PDBParser, PDBIO\n",
        "from Bio.SeqUtils import seq1\n",
        "\n",
        "import biotite.structure.io as pdb\n",
        "import biotite.structure as struct\n",
        "import numpy as np\n",
        "import csv\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
        "import gc\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    EsmForProteinFolding,\n",
        ")\n",
        "import biotite.structure.io as bsio\n",
        "\n",
        "def read_prompts(input_file):\n",
        "    atseqs = []\n",
        "    with open(input_file, encoding='utf-8-sig', newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            atseqs.append(row[0])\n",
        "    atseqs = np.array(atseqs)\n",
        "    #seq_ids = np.array(seq_ids)\n",
        "    #atseqs = [' '.join(inner_list) for inner_list in atseqs]\n",
        "    return atseqs\n",
        "\n",
        "def model_load(adapter_name):\n",
        "    model_name = \"togethercomputer/evo-1-8k-base\"\n",
        "    revision = \"1.1_fix\"\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True, revision=revision)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            config=config,\n",
        "            trust_remote_code=True, revision=revision).to(\"cuda:0\")\n",
        "    model.config.use_cache = True\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.add_special_tokens({'eos_token': ' '})\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    adapter = adapter_name\n",
        "    model.load_adapter(adapter)\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def run_model(current_aca_seqs, model, tokenizer):\n",
        "    print(current_aca_seqs)\n",
        "    input_ids = tokenizer(current_aca_seqs, return_tensors=\"pt\").to('cuda:0')\n",
        "    del input_ids['token_type_ids']\n",
        "    temp = 1.0\n",
        "    outputs = model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=1000,\n",
        "        temperature= temp,\n",
        "        repetition_penalty=None,\n",
        "        top_k=4,\n",
        "        top_p=1,\n",
        "        penalty_alpha=None,\n",
        "        do_sample=temp is not None,\n",
        "        eos_token_id=tokenizer.eos_token_id)\n",
        "    try:\n",
        "        genseq = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except UnicodeDecodeError as e:\n",
        "        print(f\"UnicodeDecodeError encountered: {e}\")\n",
        "        genseq = \"ATATATATGCGCGCGCGC\"\n",
        "    return genseq\n",
        "\n",
        "def read_evo_seqs(csv_file_path):\n",
        "    \"\"\"Reads a CSV file, extracts and cleans data from two columns, and separates sequences and prompts based on clean and non-clean conditions.\"\"\"\n",
        "    clean_data = []  # List for clean prompts and sequences\n",
        "    non_clean_data = []  # List for non-clean prompts and sequences\n",
        "    chars_to_remove = ['`', '!', '@']  # Characters to check in prompts and remove from sequences\n",
        "\n",
        "    with open(csv_file_path, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if row:  # Check if row is not empty\n",
        "                prompt = row[0]\n",
        "                sequence = row[1] if len(row) > 1 else ''  # Handle missing second column\n",
        "\n",
        "                # Remove specified characters from the sequence\n",
        "                for char in chars_to_remove:\n",
        "                    sequence = sequence.replace(char, '')\n",
        "\n",
        "                # Check if the prompt is clean and does not contain the specified characters\n",
        "                if not any(char in prompt for char in chars_to_remove):\n",
        "                    clean_data.append((prompt, sequence))\n",
        "                else:\n",
        "                    non_clean_data.append((prompt, sequence))\n",
        "\n",
        "    return clean_data, non_clean_data\n",
        "\n",
        "def get_rc(sequences,truth):\n",
        "    dna_seq = [Seq(curr_seq) for curr_seq in sequences]\n",
        "    if truth:\n",
        "        rev_dna_seq = [curr_seq.reverse_complement() for curr_seq in dna_seq]\n",
        "        #return rev_dna_seq\n",
        "        return rev_dna_seq+dna_seq\n",
        "    else:\n",
        "        return dna_seq\n",
        "\n",
        "def make_fasta(rc_sequences, prompts, output_file):\n",
        "    dna_seq_record = [SeqRecord(dna_seq, id=prompt, description=prompt) for (dna_seq, prompt) in zip(rc_sequences, prompts)]\n",
        "    with open(output_file, \"w\") as output_handle:\n",
        "        SeqIO.write(dna_seq_record, output_handle, \"fasta\")\n",
        "\n",
        "def run_prodigal(input_file, output_file, output_orf_file):\n",
        "    cmd = f'/home/cirrascale/miniconda3/envs/evo-design/bin/prodigal -i {input_file} -a {output_file} -d {output_orf_file} -p meta'\n",
        "    subprocess.run(cmd, shell=True)\n",
        "\n",
        "def filter_full_proteins(truth,input_file,output_file):\n",
        "    with open(output_file, \"w\") as out_handle:\n",
        "        for record in SeqIO.parse(input_file, \"fasta\"):\n",
        "            if truth:\n",
        "                if len(record.seq) < 500:\n",
        "                    SeqIO.write(record, out_handle, \"fasta\")\n",
        "            else:\n",
        "                SeqIO.write(record, out_handle, \"fasta\")\n",
        "\n",
        "def align_and_save_high_scores(test_sequences_file, reference_sequences_file, output_fasta_file):\n",
        "    # Create an aligner object\n",
        "    aligner = Align.PairwiseAligner()\n",
        "    # Configure the aligner to the specific needs\n",
        "    aligner.mode = 'global'  # For global alignment; use 'local' for local alignment\n",
        "    aligner.match_score = 2  # Score for matched characters\n",
        "    aligner.mismatch_score = -1  # Penalty for mismatched characters\n",
        "    aligner.open_gap_score = -0.5  # Penalty for opening a gap\n",
        "    aligner.extend_gap_score = -0.1  # Penalty for extending a gap\n",
        "\n",
        "    # Load sequences from files\n",
        "    test_sequences = list(SeqIO.parse(test_sequences_file, \"fasta\"))\n",
        "    reference_sequences = list(SeqIO.parse(reference_sequences_file, \"fasta\"))\n",
        "\n",
        "    # List to store sequences with high alignment scores\n",
        "    high_score_sequences = []\n",
        "\n",
        "    # Iterate over each test sequence\n",
        "    for test_seq in test_sequences:\n",
        "        # Variable to track the highest score relative to sequence length\n",
        "        highest_identity = 0\n",
        "\n",
        "        # Compare each test sequence against each reference sequence\n",
        "        for ref_seq in reference_sequences:\n",
        "            # Calculate the score for the current pair\n",
        "            score = aligner.score(test_seq.seq, ref_seq.seq)\n",
        "\n",
        "            # Calculate the maximum possible score for this pair (if all characters matched)\n",
        "            max_score = min(len(test_seq.seq), len(ref_seq.seq)) * aligner.match_score\n",
        "\n",
        "            # Calculate the identity percentage\n",
        "            identity_percentage = (score / max_score) * 100\n",
        "\n",
        "            # Update the highest identity found for this test sequence\n",
        "            if identity_percentage > highest_identity:\n",
        "                highest_identity = identity_percentage\n",
        "\n",
        "        # Check if the highest identity percentage for this test sequence is above 25%\n",
        "        if highest_identity >= 40:\n",
        "            # Append this test sequence to the list\n",
        "            high_score_sequences.append(test_seq)\n",
        "\n",
        "    # Write the sequences that meet the criteria to an output FASTA file\n",
        "    SeqIO.write(high_score_sequences, output_fasta_file, \"fasta\")\n",
        "\n",
        "def fold_proteins(input_file,output_path,file_name):\n",
        "    try:\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        print(f\"Directory '{output_path}' created successfully or already exists.\")\n",
        "    except OSError as error:\n",
        "        print(f\"Failed to create directory '{output_path}'. Error: {error}\")\n",
        "    esmfold = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
        "    esmfold = esmfold.to('cuda:0')\n",
        "    esmfold.esm = esmfold.esm.half()\n",
        "    esmfold_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
        "    end_file = '.pdb'\n",
        "    for i, protein_record in enumerate(SeqIO.parse(input_file, \"fasta\")):\n",
        "        protein_seq = str(protein_record.seq)[:-1] # remove stop codon\n",
        "        print('Protein sequence: ', protein_seq)\n",
        "        with torch.inference_mode():\n",
        "            esmfold_in = esmfold_tokenizer([protein_seq], return_tensors=\"pt\", add_special_tokens=False)\n",
        "            esmfold_out = esmfold(**esmfold_in.to('cuda:0'))\n",
        "            esmfold_out_pdb = esmfold.output_to_pdb(esmfold_out)[0]\n",
        "        curr_pro = str(i)\n",
        "        file_curr = output_path + '/' + file_name + curr_pro + end_file\n",
        "        print(file_curr)\n",
        "        with open(file_curr, \"w\") as f:\n",
        "            f.write(esmfold_out_pdb)\n",
        "\n",
        "def score_pdb(input_file_name,input_path,output_file,output_fasta):\n",
        "    end_file = '.pdb'\n",
        "    nfiles = len([name for name in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, name))])\n",
        "    files_seqs = []\n",
        "    with open(output_file, 'w', newline='') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(['PDB File', 'Average pLDDT Score'])\n",
        "        for i in range(0,nfiles):\n",
        "            curr_pro = str(i)\n",
        "            file_name = input_file_name + curr_pro + end_file\n",
        "            structure = pdb.load_structure(file_name, extra_fields=[\"b_factor\"])\n",
        "            if isinstance(structure, struct.AtomArrayStack):\n",
        "                av_b_factors = [np.mean(model.b_factor) for model in structure]\n",
        "                average_plddt = np.mean(av_b_factors)\n",
        "            else:\n",
        "                b_factors = structure.b_factor\n",
        "                average_plddt = np.mean(b_factors)\n",
        "            #writer.writerow([file_name, average_plddt]) --> writes all scores to seperate file\n",
        "            if average_plddt > 0.5:\n",
        "                parser = PDBParser()\n",
        "                structure = parser.get_structure(\"test\", file_name)\n",
        "                chains = {chain.id:seq1(''.join(residue.resname for residue in chain)) for chain in structure.get_chains()}\n",
        "                writer.writerow([file_name, average_plddt,chains])\n",
        "                files_seqs.append([file_name,\"\".join(list(chains.values()))])\n",
        "    seq_records = []\n",
        "    print(files_seqs)\n",
        "    for description, sequence in files_seqs:\n",
        "        # Create a SeqRecord object from the sequence string\n",
        "        seq_record = SeqRecord(Seq(sequence),\n",
        "                               id=description,  # Use the description as the record ID\n",
        "                               description='')  # Keep the description field empty if not needed\n",
        "        seq_records.append(seq_record)\n",
        "\n",
        "    # Write the list of SeqRecord objects to a FASTA file\n",
        "    SeqIO.write(seq_records, output_fasta, \"fasta\")"
      ],
      "metadata": {
        "id": "KnAc8wFVx2u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    sort_aca_seqs = read_prompts('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/at_prompts.csv')\n",
        "    coupledpromptseqscores = []\n",
        "    model, tokenizer = model_load('lsmille/lora_evo_ta_all_layers_16')\n",
        "    n_sample_per_prompt= 50\n",
        "    n_batches = len(sort_aca_seqs)\n",
        "    for j in range(0,n_sample_per_prompt):\n",
        "        for i in range(0,n_batches):\n",
        "            genseqs = run_model(sort_aca_seqs[i], model, tokenizer)\n",
        "            genseqs = [str(genseqs)]\n",
        "            #genscores = [str(score) for score in genscores]\n",
        "            currentpromptseqscores = [[prompt,seq] for prompt, seq in zip([sort_aca_seqs[i]],genseqs)]\n",
        "            print(currentpromptseqscores)\n",
        "            coupledpromptseqscores = coupledpromptseqscores + currentpromptseqscores\n",
        "    np.savetxt('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/test_gen16.csv', coupledpromptseqscores, delimiter=',', fmt='%s')\n",
        "    randprompseqsdata,atseqsdata = read_evo_seqs('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/test_gen16.csv')\n",
        "    sequences = [sequence for _, sequence in atseqsdata]\n",
        "    prompts = [prompt for prompt, _ in atseqsdata]\n",
        "    rc_sequences = get_rc(sequences,True)\n",
        "    make_fasta(rc_sequences,prompts,'/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_seqs.fasta')\n",
        "    run_prodigal('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_seqs.fasta', '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_proteins.fasta','/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_orfs.fasta')\n",
        "    filter_full_proteins(True,'/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_proteins.fasta','/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_filt_proteins.fasta')\n",
        "    align_and_save_high_scores('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_filt_proteins.fasta','/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/real_at_protein_filt_seqs.fasta','/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb_16_msa_filt_proteins.fasta')\n",
        "    fold_proteins('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15_antitoxin_seqs.fasta','/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb','lb15at_hmm_pdb')\n",
        "    fold_proteins('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15_toxin_seqs.fasta','/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb','lb15t_hmm_pdb')\n",
        "    fold_proteins('/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15_sequences_blast.fasta','/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb','lb15blast_pdb')\n",
        "    score_pdb(\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15at_hmm_pdb',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15at_scored_pdb.csv',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15at_pdb_filt.fasta')\n",
        "    score_pdb(\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15t_hmm_pdb',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15t_scored_pdb.csv',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15t_pdb_filt.fasta')\n",
        "    score_pdb(\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15blast_hmm_pdb',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15blast_scored_pdb.csv',\n",
        "        '/large_experiments/hielab/adititm/base_evo_tests/dna-gen/eval/toxin_antitoxin/lb15/pdb/lb15blast_pdb_filt.fasta')\n",
        "main()"
      ],
      "metadata": {
        "id": "gZ-HhyG-y9ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_ykbKgksz7W-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
