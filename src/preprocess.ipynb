{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "hinq2nLQvg_2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Preprocess\n",
    "### Process toxin-antitoxin (TA) genomic sequence to prepare dataset for EVO fine-tuning\n",
    "- Dataset: TADB 3.0 (https://bioinfo-mml.sjtu.edu.cn/TADB3/index.php)\n",
    "- Toxin-Antitoxin type: Type II, Experimentally validated + in silico predicted\n",
    "\n",
    "Evo: Fine-tuning requirements:\n",
    "- MAX_seq_LENGTH = 1024 # Context length for finetuning\n",
    "- Special characters:\n",
    "    - '`': Toxin start\n",
    "    - '!': Antitoxin start\n",
    "    - '@': Type II TA pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1BBSJN-vg_6"
   },
   "source": [
    "### Basic set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tC-ZBaYvg_7"
   },
   "source": [
    "Hyperparameters for Evo (8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgXtzUavvg_7"
   },
   "outputs": [],
   "source": [
    "MAX_CONTEXT_LENGTH = 1024 # Context length for finetuning (including special tokens)\n",
    "\n",
    "TA_SPECIAL_TOKEN = {\n",
    "    'T': '`', # Toxin gene\n",
    "    'AT': '!', # Antitoxin gene\n",
    "    '2' : '@' # Type II Toxin-Antitoxin\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubX8PasNvg_9"
   },
   "source": [
    "File directories and other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diIzhpVQvg_9"
   },
   "outputs": [],
   "source": [
    "# Input data paths\n",
    "at_exp_path = '../raw_data/type_II_AT_exp_nucl.fas'\n",
    "at_pre_path = '../raw_data/type_II_AT_pre_nucl.fas'\n",
    "t_exp_path = '../raw_data/type_II_T_exp_nucl.fas'\n",
    "t_pre_path = '../raw_data/type_II_T_pre_nucl.fas'\n",
    "\n",
    "genome_csv_path_exp = '../data/NCBI_genome_exp.csv'\n",
    "\n",
    "# Train dataset size\n",
    "train_size_1k = 1000\n",
    "train_size_5k = 5000\n",
    "train_size_15k = 15000\n",
    "\n",
    "# Output data path\n",
    "genome_csv_path_1k = '../data/NCBI_genome_1k.csv'\n",
    "json_output_path_1k = '../data/training_data_1k.json'\n",
    "\n",
    "genome_csv_path_5k = '../data/NCBI_genome_5k.csv'\n",
    "json_output_path_5k = '../data/training_data_5k_2.json'\n",
    "\n",
    "genome_csv_path_15k = '../data/NCBI_genome_15k.csv'\n",
    "json_output_path_15k = '../data/training_data_15k.json'\n",
    "\n",
    "# Current output path\n",
    "genome_csv_path = genome_csv_path_5k\n",
    "json_output_path = json_output_path_5k\n",
    "\n",
    "# NCBI Log in info\n",
    "email_id = 'chang.m.yun@stanford.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "q8ctZ0Hpvg_-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYMEAAc9vg_-"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from Bio import SeqIO, Entrez\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqrbefCtvg__"
   },
   "source": [
    "### Read FASTA files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKKJveipvg__"
   },
   "source": [
    "Open, parse, and store as pandas dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "H5GymcuVvhAA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def parse_description(description=str):\n",
    "    '''\n",
    "    Parse description into 'Pair #', 'Accession', 'Start', 'End', 'Species'\n",
    "    '''\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'^(\\w+)\\s+(\\S+):c?(\\d+)-(\\d+)\\s+\\[(.*?)\\]$'\n",
    "\n",
    "    match = re.match(pattern, description)\n",
    "    if match:\n",
    "        # Extract information from matched groups\n",
    "        code = match.group(1)\n",
    "        accession = match.group(2)\n",
    "        start = match.group(3)\n",
    "        end = match.group(4)\n",
    "        species = match.group(5)\n",
    "\n",
    "        # Return extracted information\n",
    "        return code, accession, start, end, species\n",
    "    else:\n",
    "        # Return None if no match found\n",
    "        return None\n",
    "\n",
    "def fasta_to_df(fasta_path=str, A_T=str, source=str, tat_type=str):\n",
    "    '''\n",
    "    Convert FASTA file into Pandas DataFrame\n",
    "    '''\n",
    "    # Define new pandas dataFrame: Source, Pair #, Accession #, Start position, End position, Sequence\n",
    "    columns = ['type', 'source', 'pair_no', 'accession', 'start', 'end', 'seq']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Open FASTA file\n",
    "    with open(fasta_path, \"r\") as fasta_handle:\n",
    "        fasta_seqio = SeqIO.parse(fasta_handle, 'fasta')\n",
    "\n",
    "        # Populate FASTA into DataFrame\n",
    "        for fasta in fasta_seqio:\n",
    "            # Identify Header, Sequence, Description in FASTA\n",
    "            header, seq, description = str(fasta.id), str(fasta.seq), str(fasta.description)\n",
    "            if description:\n",
    "                # Parse description\n",
    "                parse = parse_description(description) # Ignore species name\n",
    "\n",
    "                if parse is not None:\n",
    "                    code, accession, start, end, species = parse\n",
    "                    \n",
    "                    if A_T == 'AT':\n",
    "                        pair_no = code[2:]\n",
    "                    else:\n",
    "                        pair_no = code[1:]\n",
    "    \n",
    "                    # Append to DataFrame\n",
    "                    df = pd.concat([df, pd.DataFrame({'type': [str(tat_type)], 'source': [str(source)], 'pair_no': [pair_no], 'accession': [str(accession)], \\\n",
    "                                                        'start': [int(start)],'end': [int(end)],'seq': [str(seq)]})], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open each FASTA file and parse into pandas dataframe: at_exp_DF, T_exp_DF, AT_pre_DF, T_pre_DF\n",
    "at_exp_df = fasta_to_df(at_exp_path, 'AT', 'exp', '2') # Experimental pairs imported from prior csv\n",
    "t_exp_df = fasta_to_df(t_exp_path, 'T', 'exp','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_pre_df = fasta_to_df(at_pre_path, 'AT', 'pre', '2')\n",
    "t_pre_df = fasta_to_df(t_pre_path, 'T', 'pre','2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtKDIqn7vhAA"
   },
   "source": [
    "Pair Toxin-Antitoxin (TAT) pairs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRWpjbTdvhAB"
   },
   "outputs": [],
   "source": [
    "# Combine toxin-antitoxin pairs, based on source, pair_no, accession\n",
    "exp_paired_df = pd.merge(at_exp_df, t_exp_df, on=['type', 'source', 'pair_no', 'accession'], how='inner', suffixes=('_at','_t'))\n",
    "pre_paired_df = pd.merge(at_pre_df, t_pre_df, on=['type', 'source', 'pair_no', 'accession'], how='inner', suffixes=('_at','_t'))\n",
    "\n",
    "# Combine experimental + in silico predicted pairs\n",
    "tat_paired_df = pd.concat([exp_paired_df, pre_paired_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tat_paired_df['pair_no']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify operon strand in genome: Forward strand, Reverse strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def strand_direction(row=pd.Series, A_T=str):\n",
    "    if A_T == 'AT':\n",
    "        # Identify antitoxin gene direction\n",
    "        if row['start_at'] < row['end_at']:\n",
    "            dir = 'F'\n",
    "        elif row['start_at'] > row['end_at']:\n",
    "            dir = 'R'\n",
    "\n",
    "    elif A_T == 'T':\n",
    "        # Identify toxin gene direction\n",
    "        if row['start_t'] < row['end_t']:\n",
    "            dir = 'F'\n",
    "        elif row['start_t'] > row['end_t']:\n",
    "            dir = 'R'\n",
    "    \n",
    "    return dir\n",
    "\n",
    "# Identify strand direction: Forward vs. Reverse\n",
    "tat_paired_df['dir_at'] = tat_paired_df.apply(strand_direction, axis=1, args=('AT',))\n",
    "tat_paired_df['dir_t'] = tat_paired_df.apply(strand_direction, axis=1, args=('T',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo-CQCw0vhAB"
   },
   "source": [
    "Find operon position in genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Toxin-Antitoxin pair always in same direction\n",
    "mask = tat_paired_df['dir_at'] != tat_paired_df['dir_t']\n",
    "print(tat_paired_df.index[mask].tolist()) # If empty, Toxin-Antitoxin pair always in same direction\n",
    "\n",
    "# Exclude Toxin-Antitoxin pairs in different directions\n",
    "tat_paired_df = tat_paired_df[tat_paired_df['dir_at'] == tat_paired_df['dir_t']]\n",
    "\n",
    "# Take direction of Toxin gene as direction of operon\n",
    "tat_paired_df['dir'] = tat_paired_df['dir_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ssj445evhAB"
   },
   "outputs": [],
   "source": [
    "# Find operon position: Start, End, Length (of Operon)\n",
    "tat_paired_df['start_operon'] = tat_paired_df.apply(lambda row: min(row['start_at'], row['start_t'], row['end_at'], row['end_t']), axis=1)\n",
    "tat_paired_df['end_operon'] = tat_paired_df.apply(lambda row: max(row['start_at'], row['start_t'], row['end_at'], row['end_t']), axis=1)\n",
    "tat_paired_df['len_operon'] = tat_paired_df['end_operon'] - tat_paired_df['start_operon'] + 1\n",
    "\n",
    "'''\n",
    "# Select max operon length (= context length for all operons)\n",
    "max_len_operon = tat_paired_df['len_operon'].max()\n",
    "if max_len_operon < MAX_SEQ_LENGTH:\n",
    "    max_len = max_len_operon\n",
    "else:\n",
    "    max_len = MAX_SEQ_LENGTH\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7pqby0YvhAB"
   },
   "source": [
    "### Find genomic regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify unique genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new Pandas DataFrame with unique accession # as NCBI genome reference \n",
    "genome_df = pd.DataFrame(tat_paired_df['accession'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide genome reference into 1k, 4k, 10k \n",
    "genome_df = genome_df.sample(frac=1, random_state=1).reset_index(drop=True) # Shuffle accession; Reset index, from 0\n",
    "\n",
    "genome_df_1k = genome_df.iloc[0:train_size_1k] # Index rows 0-999\n",
    "genome_df_5k = genome_df.iloc[train_size_1k:train_size_5k] # Index rows 1000-4999\n",
    "genome_df_15k = genome_df.iloc[train_size_5k:train_size_15k] # Index rows 5000-14999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcqPBG7YvhAB"
   },
   "source": [
    "Fetch relevant genomes from NIH NCBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvFWC6A-vhAC"
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "def NCBI_genome(accession=str):\n",
    "    '''\n",
    "    Access NCBI with email to return genome: Sequence\n",
    "    '''\n",
    "    Entrez.email = email_id\n",
    "    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
    "    record = SeqIO.read(handle, \"fasta\")\n",
    "    handle.close()\n",
    "    return str(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NCBI genome sequence for each accession no.\n",
    "# genome_df_1k['seq_genome'] = genome_df_1k['accession'].apply(NCBI_genome)\n",
    "genome_df_5k['seq_genome'] = genome_df_5k['accession'].apply(NCBI_genome)\n",
    "# genome_df_15k['seq_genome'] = genome_df_15k['accession'].apply(NCBI_genome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine genome dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import existing genome data\n",
    "genome_df_exp = pd.read_csv(genome_csv_path_exp, index_col=0)\n",
    "genome_df_1k = pd.read_csv(genome_csv_path_1k, index_col=0)\n",
    "genome_df_5k = pd.read_csv(genome_csv_path_5k, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_df_out = pd.merge(genome_df_exp, genome_df_1k, on=['accession', 'seq_genome'], how='outer')\n",
    "genome_df_out = pd.merge(genome_df_out, genome_df_5k, on=['accession', 'seq_genome'], how='outer')\n",
    "# genome_df = pd.merge(genome_df_out, genome_df_10k, on=['accession', 'seq_genome'], how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export NCBI genome sequences as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write genome_df as CSV file\n",
    "genome_df_out.to_csv(genome_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import NCBI genome sequences from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genome_df = pd.read_csv(genome_csv_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYIl2F5xvhAC"
   },
   "source": [
    "Identify context length regions (+special token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign genome to each TAT pair\n",
    "tat_paired_df_out = pd.merge(tat_paired_df, genome_df_out, on=['accession'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1-3UcH_vhAC"
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "def assemble_seq_context(row=pd.Series):\n",
    "    # Tokens\n",
    "    token_type = TA_SPECIAL_TOKEN[row['type']]\n",
    "    token_t = TA_SPECIAL_TOKEN['T']\n",
    "    token_at = TA_SPECIAL_TOKEN['AT']\n",
    "\n",
    "    # Case 1: Forward strand\n",
    "    if row['dir'] == 'F':\n",
    "        # Gene sequence\n",
    "        gene_t = row['seq_genome'][row['start_t']:row['end_t']]\n",
    "        gene_at = row['seq_genome'][row['start_at']:row['end_at']]\n",
    "\n",
    "        # Case 1-A: Toxin - Antitoxin\n",
    "        # Type token + T token + Toxin gene + T token + Spacer + AT token + Antitoxin gene + AT token\n",
    "        if row['start_t'] < row['start_at']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['end_t']+1:row['start_at']-1]\n",
    "            \n",
    "            seq_context = token_type + token_t + gene_t + token_t + spacer + token_at + gene_at + token_at\n",
    "\n",
    "        # Case 1-B: Antitoxin - Toxin\n",
    "        # Type token + AT token + Antitoxin gene + Spacer + T token + Toxin gene\n",
    "        elif row['start_t'] > row['start_at']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['end_at']+1:row['start_t']-1]\n",
    "            \n",
    "            seq_context = token_type + token_at + gene_at + token_at + spacer + token_t + gene_t + token_t\n",
    "\n",
    "    # Case 2: Reverse strand\n",
    "    elif row['dir'] == 'R':\n",
    "        # Gene sequence\n",
    "        gene_t = row['seq_genome'][row['end_t']:row['start_t']]\n",
    "        gene_at = row['seq_genome'][row['end_at']:row['start_at']]\n",
    "\n",
    "        # Case 2-A: Toxin - Antitoxin\n",
    "        # AT token + Antitoxin gene + AT token + Spacer + T token + Toxin gene + T token + Type token\n",
    "        if row['end_at'] < row['end_t']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['start_at']+1:row['end_t']-1]\n",
    "\n",
    "            seq_context = token_at + gene_at + token_at + spacer + token_t + gene_t + token_t + token_type\n",
    "\n",
    "        # Case 2-B: Antitoxin - Toxin\n",
    "        # T token + Toxin gene + T token + Spacer + AT token + Antitoxin gene + AT token + Type token\n",
    "        elif row['end_at'] > row['end_t']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['start_t']+1:row['end_at']-1]\n",
    "        \n",
    "            seq_context = token_t + gene_t + token_t + spacer + token_at + gene_at + token_at + token_type\n",
    "\n",
    "    # Add spaces if context length is shorter than max_len (eg when genome length is short)\n",
    "    global MAX_CONTEXT_LENGTH\n",
    "    if len(seq_context) < MAX_CONTEXT_LENGTH:\n",
    "        num_spaces = MAX_CONTEXT_LENGTH - len(seq_context)\n",
    "        padding_3 = ' ' * num_spaces\n",
    "    else:\n",
    "        padding_3 = ''\n",
    "\n",
    "    seq_context += padding_3\n",
    "\n",
    "    return seq_context           \n",
    "\n",
    "def assemble_seq_context_onetok(row=pd.Series):\n",
    "    # Tokens\n",
    "    token_type = TA_SPECIAL_TOKEN[row['type']]\n",
    "    token_t = TA_SPECIAL_TOKEN['T']\n",
    "    token_at = TA_SPECIAL_TOKEN['AT']\n",
    "\n",
    "    # Case 1: Forward strand\n",
    "    if row['dir'] == 'F':\n",
    "        # Gene sequence\n",
    "        gene_t = row['seq_genome'][row['start_t']:row['end_t']]\n",
    "        gene_at = row['seq_genome'][row['start_at']:row['end_at']]\n",
    "\n",
    "        # Case 1-A: Toxin - Antitoxin\n",
    "        # Type token + T token + Toxin gene + T token + Spacer + AT token + Antitoxin gene + AT token\n",
    "        if row['start_t'] < row['start_at']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['end_t']+1:row['start_at']-1]\n",
    "            \n",
    "            seq_context = token_type + token_t + gene_t + spacer + token_at + gene_at\n",
    "\n",
    "        # Case 1-B: Antitoxin - Toxin\n",
    "        # Type token + AT token + Antitoxin gene + Spacer + T token + Toxin gene\n",
    "        elif row['start_t'] > row['start_at']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['end_at']+1:row['start_t']-1]\n",
    "            \n",
    "            seq_context = token_type + token_at + gene_at + spacer + token_t + gene_t\n",
    "\n",
    "    # Case 2: Reverse strand\n",
    "    elif row['dir'] == 'R':\n",
    "        # Gene sequence\n",
    "        gene_t = row['seq_genome'][row['end_t']:row['start_t']]\n",
    "        gene_at = row['seq_genome'][row['end_at']:row['start_at']]\n",
    "\n",
    "        # Case 2-A: Toxin - Antitoxin\n",
    "        # AT token + Antitoxin gene + AT token + Spacer + T token + Toxin gene + T token + Type token\n",
    "        if row['end_at'] < row['end_t']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['start_at']+1:row['end_t']-1]\n",
    "\n",
    "            seq_context =  gene_at + token_at + spacer + gene_t + token_t + token_type\n",
    "\n",
    "        # Case 2-B: Antitoxin - Toxin\n",
    "        # T token + Toxin gene + T token + Spacer + AT token + Antitoxin gene + AT token + Type token\n",
    "        elif row['end_at'] > row['end_t']:\n",
    "            # Spacer sequence\n",
    "            spacer = row['seq_genome'][row['start_t']+1:row['end_at']-1]\n",
    "        \n",
    "            seq_context =  gene_t + token_t + spacer + gene_at + token_at + token_type\n",
    "\n",
    "    # Add spaces if context length is shorter than max_len (eg when genome length is short)\n",
    "    global MAX_CONTEXT_LENGTH\n",
    "    if len(seq_context) < MAX_CONTEXT_LENGTH:\n",
    "        num_spaces = MAX_CONTEXT_LENGTH - len(seq_context)\n",
    "        padding_3 = ' ' * num_spaces\n",
    "    else:\n",
    "        padding_3 = ''\n",
    "\n",
    "    seq_context += padding_3\n",
    "\n",
    "    return seq_context                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding + operon + special token\n",
    "# tat_paired_df_out['seq_context'] = tat_paired_df_out.apply(assemble_seq_context, axis=1)\n",
    "tat_paired_df_out['seq_context'] = tat_paired_df_out.apply(assemble_seq_context_onetok, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "hkjCIaKVvhAD",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Export as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_WE07CyvhAD"
   },
   "outputs": [],
   "source": [
    "# tat_paired_df.to_csv(csv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xep_6VS0vhAD"
   },
   "source": [
    "### Export as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TGGpFgLvhAD"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def write_json(data, OUTPUT_PATH):\n",
    "    with open(OUTPUT_PATH, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "def convert_df_to_list(df=pd.DataFrame):\n",
    "    records = []\n",
    "    global MAX_CONTEXT_LENGTH\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Append rows with context length less than or equal to max context length\n",
    "        if len(row['seq_context']) <= MAX_CONTEXT_LENGTH:\n",
    "            records.append({\n",
    "                'record': row['pair_no'],\n",
    "                'text': row['seq_context']\n",
    "                })\n",
    "\n",
    "    return records\n",
    "\n",
    "# Convert Pandas DataFrame into list\n",
    "data_list = convert_df_to_list(tat_paired_df_out)\n",
    "print('Num TA pairs: ' + str(len(data_list))) # Number of TA pairs in training data\n",
    "\n",
    "# Write JSON file from list\n",
    "write_json(data_list, json_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "bCTXqPVyvhAE",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiGQrD8-vhAE"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# From Evo CRISPR training\n",
    "def parse_fasta_with_biopython(fname, id_to_cas_id):\n",
    "    records = []\n",
    "\n",
    "    if fname.endswith('.gz'):\n",
    "        file_ = gzip.open(fname, 'rt')\n",
    "    else:\n",
    "        file_ = open(fname, 'r')\n",
    "\n",
    "    for record in SeqIO.parse(file_, 'fasta'):\n",
    "        seq = str(record.seq)\n",
    "\n",
    "        if 'NNN' in seq:\n",
    "            continue\n",
    "\n",
    "        if len(seq) >= MAX_seq_LENGTH - 2: # Minus start and end tokens.\n",
    "            extra = len(seq) - (MAX_seq_LENGTH - 2)\n",
    "            seq = seq[:-extra]\n",
    "\n",
    "        cas_id = id_to_cas_id[record.id]\n",
    "\n",
    "        seq = CAS_ID_TO_START_TOKEN[cas_id] + seq # Encode start token.\n",
    "        # Stop token (EOD) is appended in downstream preprocess_data.py script.\n",
    "\n",
    "        records.append({\n",
    "            'record': record.id,\n",
    "            'text': seq,\n",
    "        })\n",
    "\n",
    "    file_.close()\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def write_json(fname, data, output_path):\n",
    "    with open(output_path, 'at') as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "\n",
    "def process_file(fname, id_to_cas_id, output_file):\n",
    "    parsed_data = parse_fasta_with_biopython(fname, id_to_cas_id)\n",
    "    write_json(fname, parsed_data, output_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Parse Cas sequences and output JSONL.\")\n",
    "    parser.add_argument(\"fasta_path\", type=str,\n",
    "                        help=\"Path to directory containing input FASTA file.\")\n",
    "    parser.add_argument(\"metadata_path\", type=str,\n",
    "                        help=\"Path to Cas metadata file.\")\n",
    "    parser.add_argument(\"output_path\", type=str,\n",
    "                        help=\"Path to output JSON file.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    id_to_cas_id = load_metadata(args.metadata_path)\n",
    "\n",
    "    process_file(args.fasta_path, id_to_cas_id, args.output_path)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
